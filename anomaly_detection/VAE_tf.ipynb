{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Variational Autoencoders\n",
    "### From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    \n",
    "    def __init__(self, nnArch, transfer=tf.nn.softplus, lrate=0.001, bSize=100):\n",
    "        self.nnArch = nnArch\n",
    "        self.transfer = transfer\n",
    "        self.lrate = lrate\n",
    "        self.bSize = bSize\n",
    "        self.x = tf.placeholder(tf.float32, [None, nnArch[\"nInput\"]]) \n",
    "        \n",
    "        self.createNN()\n",
    "        self.createLossOpt() \n",
    "\n",
    "        init = tf.global_variables_initializer() \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def createNN(self):\n",
    "        weights = self.initWeights(**self.nnArch)\n",
    "        self.zMu, self.zLogS2 = self.recNN(weights[\"Wrec\"], weights[\"brec\"]) \n",
    "        nZ = self.nnArch[\"nZ\"] # Draw one sample z from Gaussian distribution\n",
    "        eps = tf.random_normal((self.bSize, nZ), 0, 1, dtype=tf.float32)\n",
    "        self.z = tf.add(self.zMu, tf.multiply(tf.sqrt(tf.exp(self.zLogS2)), eps)) # z = mu + sigma*epsilon\n",
    "        self.xRecMean = self.genNN(weights[\"Wgen\"], weights[\"bgen\"]) # Use generator to determine mean of Bernoulli distribution of reconstructed input\n",
    "            \n",
    "    def xInit(self, fan_in, fan_out, constant=1): \n",
    "        low = -constant * np.sqrt(6.0/(fan_in + fan_out)) \n",
    "        high = constant * np.sqrt(6.0/(fan_in + fan_out))\n",
    "        return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "    def initWeights(self, nHrec1, nHrec2, nHgen1,  nHgen2, nInput, nZ):\n",
    "        weights = dict()\n",
    "        weights['Wrec'] = {\n",
    "            'h1': tf.Variable(self.xInit(nInput, nHrec1)),\n",
    "            'h2': tf.Variable(self.xInit(nHrec1, nHrec2)),\n",
    "            'outMean': tf.Variable(self.xInit(nHrec2, nZ)),\n",
    "            'outLogS': tf.Variable(self.xInit(nHrec2, nZ))}\n",
    "        weights['brec'] = {\n",
    "            'b1': tf.Variable(tf.zeros([nHrec1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([nHrec2], dtype=tf.float32)),\n",
    "            'outMean': tf.Variable(tf.zeros([nZ], dtype=tf.float32)),\n",
    "            'outLogS': tf.Variable(tf.zeros([nZ], dtype=tf.float32))}\n",
    "        weights['Wgen'] = {\n",
    "            'h1': tf.Variable(self.xInit(nZ, nHgen1)),\n",
    "            'h2': tf.Variable(self.xInit(nHgen1, nHgen2)),\n",
    "            'outMean': tf.Variable(self.xInit(nHgen2, nInput)),\n",
    "            'outLogS': tf.Variable(self.xInit(nHgen2, nInput))}\n",
    "        weights['bgen'] = {\n",
    "            'b1': tf.Variable(tf.zeros([nHgen1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([nHgen2], dtype=tf.float32)),\n",
    "            'outMean': tf.Variable(tf.zeros([nInput], dtype=tf.float32)),\n",
    "            'outLogS': tf.Variable(tf.zeros([nInput], dtype=tf.float32))}\n",
    "        return weights\n",
    "            \n",
    "    # Generate probabilistic encoder (recognition network), which maps inputs onto a normal distribution in latent space. The transformation is parametrized and can be learned. Use recognition network to determine mean and (log) variance of Gaussian distribution in latent space\n",
    "    def recNN(self, weights, biases):\n",
    "        layer1 = self.transfer(tf.add(tf.matmul(self.x, weights['h1']), biases['b1'])) \n",
    "        layer2 = self.transfer(tf.add(tf.matmul(layer1, weights['h2']), biases['b2'])) \n",
    "        zMu = tf.add(tf.matmul(layer2, weights['outMean']), biases['outMean'])\n",
    "        zLogS2 = tf.add(tf.matmul(layer2, weights['outLogS']), biases['outLogS'])\n",
    "        return (zMu, zLogS2)\n",
    "\n",
    "    # Generate probabilistic decoder (decoder network), which maps points in latent space onto a Bernoulli distribution in data space. The transformation is parametrized and can be learned.\n",
    "    def genNN(self, weights, biases):\n",
    "        layer1 = self.transfer(tf.add(tf.matmul(self.z, weights['h1']), biases['b1'])) \n",
    "        layer2 = self.transfer(tf.add(tf.matmul(layer1, weights['h2']), biases['b2'])) \n",
    "        xRecMean = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weights['outMean']), biases['outMean']))\n",
    "        return xRecMean\n",
    "            \n",
    "    # Define loss function based variational upper-bound and corresponding optimizer. The loss is composed of two terms:\n",
    "    # 1 Reconstruction loss (negative log probability of the input under the reconstructed Bernoulli distribution induced by the decoder in the data space) i.e. the number of \"nats\" required for reconstructing the input when  activation in latent is given\n",
    "    # 2.Latent loss: the Kullback Leibler divergence between the distribution in latent space induced by the encoder on the data and some prior. This acts as a kind of regularizer. This can be interpreted as the number of \"nats\" required for transmitting the the latent space distribution given the prior.\n",
    "    def createLossOpt(self):\n",
    "        recLoss = -tf.reduce_sum(self.x * tf.log(1e-10 + self.xRecMean) + (1-self.x) * tf.log(1e-10 + 1 - self.xRecMean),1)\n",
    "        latLoss = -0.5 * tf.reduce_sum(1 + self.zLogS2 - tf.square(self.zMu) - tf.exp(self.zLogS2), 1)\n",
    "        self.cost = tf.reduce_mean(recLoss + latLoss)  \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lrate).minimize(self.cost)\n",
    "        \n",
    "    #Train model based on mini-batch of input data. Return cost of batch\n",
    "    def Fit(self, X):\n",
    "        return self.sess.run((self.optimizer, self.cost), feed_dict={self.x: X})\n",
    "    \n",
    "    # Transform data by mapping it into the latent space\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.zMu, feed_dict={self.x: X})\n",
    "    \n",
    "    # Generate data by sampling from latent space\n",
    "    def generate(self, zMu):\n",
    "        return self.sess.run(self.xRecMean, feed_dict={self.z: zMu})\n",
    "    \n",
    "    # Reconstruct original data\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.xRecMean, feed_dict={self.x: X})\n",
    "\n",
    "    def train(self, dataset, epochs):\n",
    "        printStep = 5\n",
    "        for epoch in range(epochs):\n",
    "            cost = self.Fit(dataset)\n",
    "            if epoch % printStep == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(cost))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test VAE\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'read_data_sets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-12af68adfd4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Test VAE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnnArch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnHrec1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnHrec2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnHgen1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnHgen2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnInput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnZ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'read_data_sets'"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "print (\"Test VAE\")\n",
    "id.read_data_sets()\n",
    "mnist = id.read_data_sets('MNIST_data', one_hot=True)\n",
    "nnArch = dict(nHrec1=500, nHrec2=500, nHgen1=500, nHgen2=500, nInput=784, nZ=20)\n",
    "self.train(nnArch, mnist, epochs=5, nSamples=mnist.train.num_examples)\n",
    "pass\n",
    "\n",
    "def testName(self):\n",
    "    df = pd.read_csv('TsToyAnomaly.csv', sep='\\t', header=None)\n",
    "    dataset = df.transpose()\n",
    "    nnArch = dict(nHrec1=25, nHrec2=25, nHgen1=25, nHgen2=25, nInput=50, nZ=20)\n",
    "    vae = Vae.VAE(nnArch, lrate=0.001, bSize=100)\n",
    "    vae.train(epochs=10)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
